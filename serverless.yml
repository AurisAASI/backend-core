org: acsfti

service: backend-core

frameworkVersion: '4'

provider:
  name: aws
  runtime: python3.10
  stage: ${opt:stage, 'dev'}
  region: ${opt:region, 'us-east-1'}
  memorySize: 128
  timeout: 30
  environment:
    # Global environment variables
    ACCOUNT_ID: ${env:AWS_ACCOUNT_ID, '819774487459'}
    STAGE: ${self:provider.stage}
    REGION: ${self:provider.region}
    # API Key from AWS Cognito stored in environment
    API_KEY: ${env:API_KEY, ''}
    # Google Places API configuration (stage-specific)
    GOOGLE_PLACES_API_KEY_DEV: ${env:GOOGLE_PLACES_API_KEY_DEV, ''}
    GOOGLE_PLACES_API_KEY_PROD: ${env:GOOGLE_PLACES_API_KEY_PROD, ''}
    GOOGLE_PLACES_DAILY_QUOTA_LIMIT_DEV: ${env:GOOGLE_PLACES_DAILY_QUOTA_LIMIT_DEV, '10000'}
    GOOGLE_PLACES_DAILY_QUOTA_LIMIT_PROD: ${env:GOOGLE_PLACES_DAILY_QUOTA_LIMIT_PROD, '20000'}
    # Google Gemini API configuration (stage-specific)
    GEMINI_API_KEY_DEV: ${env:GEMINI_API_KEY_DEV, ''}
    GEMINI_API_KEY_PROD: ${env:GEMINI_API_KEY_PROD, ''}
  apiGateway:
    # Enable API Gateway API keys
    apiKeys:
      - name: ${self:service}-${self:provider.stage}-key
        description: API key for ${self:service} service in ${self:provider.stage} stage
        value: ${env:API_KEY, ''}  # Use value from environment variables
    usagePlan:
      throttle:
        burstLimit: 200
        rateLimit: 100
  iam:
    role:
      statements:
        # Default permissions for all functions
        - Effect: Allow
          Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
          Resource: "arn:aws:logs:*:*:*"

package:
  individually: true
  patterns:
    - '!node_modules/**'
    - '!.venv/**'
    - '!venv/**'
    - '!.gitignore'
    - '!.git/**'
    - '!__pycache__/**'
    - '!tests/**'
    - '!.pytest_cache/**'
    - '!.vscode/**'
    - '!htmlcov/**'
    - '!.github/**'
    - '.flake8'
    - '!.env.example'
    - '!.coverage*'
    - '!README.md'

functions:
  city_collector:
    handler: src/functions/city_collector/handler.city_collector
    description: City Collector Lambda function
    timeout: 29
    events:
      - http:
          path: city_collector
          method: post
          cors: true
          private: true  # Require API key
      - eventBridge:
          pattern:
            source:
              - custom.cityCollector
            detail-type:
              - City Collection Request
    package:
      patterns:
        - src/functions/city_collector/**
        - src/shared/**
    environment:
      # Function specific environment variables
      FUNCTION_NAME: city_collector
      SCRAPER_TASK_QUEUE_URL: !Ref ScraperTaskQueue
    iamRoleStatements:
      - Effect: Allow
        Action:
          - sqs:SendMessage
        Resource: !GetAtt ScraperTaskQueue.Arn
    layers:
      - arn:aws:lambda:${self:provider.region}:${env:AWS_ACCOUNT_ID}:layer:city_collector_layer:1
      - arn:aws:lambda:${self:provider.region}:017000801446:layer:AWSLambdaPowertoolsPythonV3-python310-x86_64:19

  data_scrapper:
    handler: src/functions/data_scrapper/handler.data_scrapper
    description: Data Scrapper Lambda function that processes scraping tasks
    timeout: 300
    memorySize: 512
    events:
      - sqs:
          arn: !GetAtt ScraperTaskQueue.Arn
          batchSize: 1
    package:
      patterns:
        - src/functions/data_scrapper/**
        - src/shared/**
        - src/models/**
    environment:
      FUNCTION_NAME: data_scrapper
      COMPANIES_TABLE: ${self:provider.stage}-auris-core-companies
      PLACES_TABLE: ${self:provider.stage}-auris-core-places
      WEBSITE_SCRAPER_TASK_QUEUE_URL: !Ref WebsiteScraperTaskQueue
    iamRoleStatements:
      - Effect: Allow
        Action:
          - dynamodb:GetItem
          - dynamodb:PutItem
          - dynamodb:UpdateItem
          - dynamodb:Scan
          - dynamodb:Query
        Resource:
          - arn:aws:dynamodb:${self:provider.region}:${env:AWS_ACCOUNT_ID}:table/${self:provider.stage}-auris-core-places
          - arn:aws:dynamodb:${self:provider.region}:${env:AWS_ACCOUNT_ID}:table/${self:provider.stage}-auris-core-companies
          - arn:aws:dynamodb:${self:provider.region}:${env:AWS_ACCOUNT_ID}:table/${self:provider.stage}-auris-core-places/index/*
          - arn:aws:dynamodb:${self:provider.region}:${env:AWS_ACCOUNT_ID}:table/${self:provider.stage}-auris-core-companies/index/*
      - Effect: Allow
        Action:
          - sqs:SendMessage
        Resource: !GetAtt WebsiteScraperTaskQueue.Arn
    layers:
      - arn:aws:lambda:${self:provider.region}:017000801446:layer:AWSLambdaPowertoolsPythonV3-python310-x86_64:19

  website_scrapper:
    handler: src/functions/data_scrapper/website_handler.website_scrapper
    description: Website Scrapper Lambda function that processes website scraping tasks
    timeout: 300
    memorySize: 512
    events:
      - sqs:
          arn: !GetAtt WebsiteScraperTaskQueue.Arn
          batchSize: 1
    package:
      patterns:
        - src/functions/data_scrapper/**
        - src/shared/**
        - src/models/**
    environment:
      FUNCTION_NAME: website_scrapper
      COMPANIES_TABLE: ${self:provider.stage}-auris-core-companies
    iamRoleStatements:
      - Effect: Allow
        Action:
          - dynamodb:GetItem
          - dynamodb:PutItem
          - dynamodb:UpdateItem
          - dynamodb:Query
        Resource:
          - arn:aws:dynamodb:${self:provider.region}:${env:AWS_ACCOUNT_ID}:table/${self:provider.stage}-auris-core-companies
          - arn:aws:dynamodb:${self:provider.region}:${env:AWS_ACCOUNT_ID}:table/${self:provider.stage}-auris-core-companies/index/*
    layers:
      - arn:aws:lambda:${self:provider.region}:017000801446:layer:AWSLambdaPowertoolsPythonV3-python310-x86_64:19

plugins:
  - serverless-offline
  - serverless-add-api-key

custom:
  apiKey:
    name: ${self:service}-${self:provider.stage}-api-key
    description: API key for ${self:service} service in ${self:provider.stage} stage
  pythonRequirements:
    dockerizePip: false
    layer: false
    slim: true
    useDownloadCache: true
    useStaticCache: true
    pythonBin: python3.10

resources:
  Resources:
    # DynamoDB Tables are managed externally (already exist)
    # Tables: ${self:provider.stage}-auris-core-companies, ${self:provider.stage}-auris-core-places

    # SQS Queues
    ScraperTaskQueue:
      Type: AWS::SQS::Queue
      Properties:
        QueueName: ${self:service}-${self:provider.stage}-scraper-tasks
        VisibilityTimeout: 300
        MessageRetentionPeriod: 86400  # 1 day
        ReceiveMessageWaitTimeSeconds: 0
        RedrivePolicy:
          deadLetterTargetArn: !GetAtt ScraperTaskDLQ.Arn
          maxReceiveCount: 3  # After 3 failed attempts, move to DLQ
        Tags:
          - Key: Service
            Value: ${self:service}
          - Key: Stage
            Value: ${self:provider.stage}

    ScraperTaskDLQ:
      Type: AWS::SQS::Queue
      Properties:
        QueueName: ${self:service}-${self:provider.stage}-scraper-tasks-dlq
        MessageRetentionPeriod: 1209600  # 14 days
        Tags:
          - Key: Service
            Value: ${self:service}
          - Key: Stage
            Value: ${self:provider.stage}

    ScraperAlarmTopic:
      Type: AWS::SNS::Topic
      Properties:
        TopicName: ${self:service}-${self:provider.stage}-scraper-alerts
        DisplayName: Scraper Task Failure Alerts
        Tags:
          - Key: Service
            Value: ${self:service}
          - Key: Stage
            Value: ${self:provider.stage}

    ScraperAlarmTopicSubscription:
      Type: AWS::SNS::Subscription
      Properties:
        Protocol: email
        TopicArn: !Ref ScraperAlarmTopic
        Endpoint: ${env:ALARM_EMAIL, 'your-email@example.com'}

    ScraperTaskDLQAlarm:
      Type: AWS::CloudWatch::Alarm
      Properties:
        AlarmName: ${self:service}-${self:provider.stage}-scraper-dlq-alarm
        AlarmDescription: Alert when messages are sent to the DLQ (scraping failures)
        MetricName: ApproximateNumberOfMessagesVisible
        Namespace: AWS/SQS
        Statistic: Sum
        Period: 60
        EvaluationPeriods: 1
        Threshold: 1
        ComparisonOperator: GreaterThanOrEqualToThreshold
        Dimensions:
          - Name: QueueName
            Value: !GetAtt ScraperTaskDLQ.QueueName
        TreatMissingData: notBreaching
        AlarmActions:
          - !Ref ScraperAlarmTopic

    # Website Scraper SQS Queues
    WebsiteScraperTaskQueue:
      Type: AWS::SQS::Queue
      Properties:
        QueueName: ${self:service}-${self:provider.stage}-website-scraper-tasks
        VisibilityTimeout: 360  # 6 minutes (higher than Lambda timeout)
        MessageRetentionPeriod: 86400  # 1 day
        ReceiveMessageWaitTimeSeconds: 0
        RedrivePolicy:
          deadLetterTargetArn: !GetAtt WebsiteScraperTaskDLQ.Arn
          maxReceiveCount: 2  # After 2 failed attempts, move to DLQ
        Tags:
          - Key: Service
            Value: ${self:service}
          - Key: Stage
            Value: ${self:provider.stage}

    WebsiteScraperTaskDLQ:
      Type: AWS::SQS::Queue
      Properties:
        QueueName: ${self:service}-${self:provider.stage}-website-scraper-tasks-dlq
        MessageRetentionPeriod: 1209600  # 14 days
        Tags:
          - Key: Service
            Value: ${self:service}
          - Key: Stage
            Value: ${self:provider.stage}

    WebsiteScraperTaskDLQAlarm:
      Type: AWS::CloudWatch::Alarm
      Properties:
        AlarmName: ${self:service}-${self:provider.stage}-website-scraper-dlq-alarm
        AlarmDescription: Alert when messages are sent to the website scraper DLQ (scraping failures)
        MetricName: ApproximateNumberOfMessagesVisible
        Namespace: AWS/SQS
        Statistic: Sum
        Period: 60
        EvaluationPeriods: 1
        Threshold: 1
        ComparisonOperator: GreaterThanOrEqualToThreshold
        Dimensions:
          - Name: QueueName
            Value: !GetAtt WebsiteScraperTaskDLQ.QueueName
        TreatMissingData: notBreaching
        AlarmActions:
          - !Ref ScraperAlarmTopic
